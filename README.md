# The Messiness of Politics: Wrangling U.S. Presidential Election Data from 1976-2012



## Introduction & Problem Motivation: 

The United States has long been known for its strong political environment. With its roots deriving as the birthplace of democracy, America takes its political elections seriously and stands for representation for and by the people. As such, understanding election data is critical, especially when considering national presidential elections where the highest position of the government is at stake. Americans in particular are known to align themselves closely with their political party, with increased engagement and identification occurring during presidential election seasons. For example, a study conducted by the University of Stanford found that political partisanship acts as a stronger social divide compared to other identification factors such as race, ethnicity, and religion (Westwood et. al, 2017). Previous research also shows patterns of increased polarization and movement of political parties over the years (Pew Research Center, 2014). As such, this information remains highly relevant. Politicians, analysts, and political party donors can use prior election results data to predict future election outcomes, helping them to understand focus areas regarding resources and outreach efforts. Considering the personal investment from citizens, this knowledge can also be valuable for citizens to understand how to extend greater awareness of their respective political parties. Given this background, we were motivated to take the approach of working as a political data science team with the following key questions: (1) Which political parties have historically won presidential elections? (2) What has the breakdown of total votes been across different political parties in presidential elections? (3) Does the breakdown of total votes for political parties change by year and state? 


While we did not go into the project with a specific time frame in mind, we had hoped for data spanning at least a decade with at least five different presidential elections represented. We employed the data science framework discussed in class (Fig. 1), focusing mainly on the areas of (1) Ask an interesting question; and (2) Get the data. We will participate mildly in the third piece of the framework (3) Explore the data, recognizing that this phase more heavily touches the area of analyst teams interested in finding patterns and drawing data conclusions.



## Data Collection Process:

During the data collection phase, our team initially brainstormed a range of dataset ideas across various domains in an in-person session. We considered multiple topics where data wrangling could be meaningfully applied including the following five areas: (1) Travel (e.g., airline or tourism statistics); (2) Movies (e.g., film ratings or box office data); (3) Restaurants (e.g., customer reviews or restaurant health inspections); (4) Elections (e.g., voter turnout records or election results); and (4) College Rankings (e.g., university ranking metrics). For us, each of these areas were not only relevant to Americans, but also played key roles in shaping culture and society. With this in mind, we continued our asking phase by practically searching for datasets across these areas that would provide reasonable datasets to work with, thus helping us to narrow down our topic organically by the actual data available. Our goal was to find a dataset that was rich enough to require substantial cleaning and transformation, but not so pre-processed that there would not be enough for us to do. We documented promising datasets and project ideas, serving as our initial exploratory analysis. This collaborative brainstorming allowed us to weigh each option’s feasibility and potential insights, ensuring we chose a dataset well-suited for a comprehensive data wrangling project.


While we had discovered an interesting dataset looking at movie reviews, we ultimately found election-focused data to be most interesting to us, especially considering that other groups we talked to were not researching this space, and considering the anticipation of November as the traditional election season. With an election-focused approach, our favorite early idea was to understand voter turnout using CNN’s 2024 exit poll data. This exit poll information seemed exciting, given its detailed insights into a wide variety of voter demographics and behavior in the most recent presidential election. However, we encountered significant challenges in retrieving the data, given its JSON file format. This would have required a significant amount of time, expertise, and likely computing power to load and parse the data into a usable format. Just this investment alone could have fulfilled the wrangling requirement of our project. However, we were more interested in actually interacting with the cleaning elements of the dataset rather than ‘simply’ loading the data. After determining that extracting and cleaning this data within our project’s timeframe would be impractical, we pivoted as a team to explore more accessible sources of historical election data. In collaboration with an identified external expert and scholar of data science (i.e. our professor), we found a website called Harvard Dataverse that offered election datasets from MIT Election Data + Science Lab. While this source provided a variety of election data, we chose one that aligned with our interest in U.S. presidential elections rather than legislative elections, for instance. The chosen dataset, entitled “U.S. President 1976-2020,” contained total votes for each U.S. state during U.S. presidential elections from 1976 to 2020, with 4,288 rows and 15 columns.



## Data Wrangling Process: 

* Phase 1 – Data Loading and Initial Preparation: The entire data cleaning process was done in Visual Studio Code in a Quarto document. We imported the dataset ‘1976-2020-president’ into the quarto document using pandas, and an initial assessment of the loaded data was made(pandas: .describe()) (Table 3). The data initially had inconsistent datatypes, where columns such as year and number of votes were set as strings. The column names were also not entirely self-explanatory, and some columns were not relevant for the questions at hand (see ‘Introduction’) and for future analysis. Moreover, the data’s formatting was also inconsistent, with some columns being entirely capitalized while others contained special characters. Lastly, there were a significant number of missing values in some of the relevant columns that would later be required for the analysis.


* Phase 2 – Data Format Cleaning: Using the pandas .drop() function, we dropped columns that were irrelevant to our questions, did not contain actual data for their respective rows, and ones that contained redundant information (Table 1). To check for missing values, we used pandas .isnull().sum() function. We observed that the “party_detailed” column contained 456 missing values and the “candidate” column contained 287 missing values. While the missing values in “candidate” would not necessarily impact our future analysis since we were dropping this column, the missing values in “party_detailed” would, as all these missing “party_detailed” values were being grouped into the “others” category in the “party_simplified” column. This could potentially skew the final output towards the “others” segment. Therefore, we decided to drop all rows that contained a missing value in the party_detailed column using pandas .isnull().sum(). With only the desired columns remaining, we then renamed the columns for better clarity and readability, recognizing that we would be passing off our work to another team eventually (Table 2). 

To change the unmatched data types, we used the pandas function pd.numeric(), converting the columns “year,” “candidate_votes,” and “total_votes” from string to a numerical data type, and the “writein” column was converted from string to a boolean data type to make calculations and filtering easier. Next, some of the data was standardized. For this process, we created a function that ensured, firstly, columns “state,” “party_detailed,” “party_simplified,” and “candidate_name” were read as text. Then the function removed any unnecessary whitespace and converted all entries to a consistent Title Case format.

The next major formatting step that we decided to take was splitting the candidate's name into first and last names. The original candidate column was in the format: “Last name, First name “Nickname””. To improve the formatting, we created a function using pandas pd.isna() that would use a regular expression (library imported: re) to extract the first and last names. This process extracted the first and last name from the column “candidate_name,” populated two new columns, “candidate_lastname” and “candidate_firstname,” and deleted the original candidate column (Table 4). 

* Phase 3 – Creating Derived Columns: To help interpret the data and the final output better, we created a derived column called voting_percentage. To calculate the voting percentage, we created a function that takes the “candidate_votes,” divides it by the “total_votes,” and multiplies it by 100. We then rounded this percentage value to two decimal points and stored it in a new column called “vote_percentage”

* Phase 4 – Pivoting: The final step to wrangling our data involved creating three different pivot tables using pd.pivot_table. The first pivot table, entitled “Simplified Pivot Table,” was built after identifying the candidate with the most votes per year using pandas groupby() and idxmax(). This filtered data was then pivoted, where we chose to use “year” as the index, as we identified this column as being a unique identifier for each row, and as we wanted to be able to see trends across different election cycles. We chose “party_simplified” as the column so we could obtain a high-level overview of the winning candidate's political party from either Democrat, Republican, Libertarian, or Other. The winning “candidate_votes” and “vote_percentage” were stored (Table 5). To validate that this table was correctly pivoted, we used matplotlib.pyplot to create a bar chart, showing the number of election wins by party, with Democrats in blue and Republicans in red (Fig. 1).

To show a breakdown of votes by political party for each year rather than just that of the winning party, we built another pivot table called “Detailed Pivot Table” using again the election “year” as the unique index and “party_simplified” as columns for the same previously explained reasons as our first pivot table. It aggregated “total_votes” with sum and “vote_percentage” with mean, producing a year-by-party summary of overall voting patterns (Table 6). To verify our pivot table, we created a bar chart visualization using matplotlib.pyplot, which matched how we anticipated the data to look (Fig. 2).

Lastly, to understand which political party won the election at the state level, we created a pivot table called “Interactive State-Level Pivot Table (All Years)” using the library plotly.express. By using this method, we aimed to create a table where, by selecting a year from a dropdown menu, the table dynamically filtered the state winners dataset (built with pandas groupby and idxmax function) and pivoted it by “state.” The resulting table displayed the winning “party_simplified” and the “vote_percentage” for each state, allowing us to explore across different election years.
For the sanity check, using the plotly library, we created an interactive choropleth map (plotly.express) showing state-level winners by year to check that our pivot worked (Fig. 4). This visualization includes a dropdown (Plotly’s updatemenus feature) for year selection that dynamically updates the map (Fig. 4). 



## Conclusions/Deliverables: 

Based on the data, initial observations (Fig. 2, Fig. 3, and Fig. 4) confirm that our cleaning process worked, with the bar charts and the map achieved by the end of the pipeline reflecting the data breakdown properly. For example, Fig. 2 reflects the overall number of wins of the simplified political parties (i.e., either democrat, republican, or libertarian) across the twelve elections, showing democrat candidates winning the election eight times and republican candidates winning four times, which matches our dataset. Fig. 3 reflects a more specific breakdown by providing a delineation of total votes by each presidential election year from 1976-2020, showing how many votes each simplified party received. It appears that in some years one political party dominated, such as 1984 and 1996, while in other years there was a greater split across the parties for the number of votes, as seen in 2012 and 2020, for example. Moreover, we also saw that over the years, the trend of different states has also significantly changed, particularly across different regions, such as the West coast voting primarily Republican in the 1970s, but then shifting toward Democrat leaning by the 2000s with the reverse trend occurring in the South (Fig. 4). 


Additionally, our team really enjoyed working through this project, particularly given that our scope fell within the front-end of a typical data science role (i.e. (1) Ask an interesting question and (2) Get the data) while our Machine Learning course taken conjointly focuses primarily on the back-end of this type of project of exploring, modeling, and communicating the data results and patterns. One of our learnings involved understanding the importance of identifying the key question. This proved important during our ideation phase, where we consistently found various datasets, yet had to collaborate with each other on deciding whether the dataset actually aligned with a question we found to be interesting. In identifying the key question, we also discovered how this serves as the basis for the data cleaning process and methods employed. For example, when working through pivoting our table, we realized that we were employing different coding methods as we assumed different desired outputs, with one of us retrieving detailed political party breakdown information compared to the other looking at the simplified political party breakdown. As such, this prompted us to return to our initial project outline, where we documented our main questions. Realizing our questions were focused on a more macro-view centered around political party victory and votes more generally rather than by a detailed political party breakdown with all possible parties included, we decided to forgo the coding method required to look at the breakdown by all the political parties. During the phase of obtaining the data, we learned about the extremities and nuances that exist among retrieving datasets. While we had initially hypothesized this would be the simpler part as compared to the actual cleaning of the data, we realized that this proved equally, if not more, difficult. For instance, as discussed earlier, our exploratory phase involved finding a dataset that we had hoped to use yet the JSON file format proved unsuitable for the scope of our project, given the significant investment of parsing and loading in the data. We now have a greater appreciation for datasets that we can seemingly ‘simply’ download. Additionally, when exercising the coding skill of pivoting tables, we learned about the interactions between the values within the dataset. For example, we learned about identifying a column that could act as the unique key for the table to pivot on. We also discovered this relational nature of the values within a table through observing the impact of when we choose to drop and filter certain columns. For instance, we noticed that when we filtered the party_detailed column, it impacted whether or not data appeared within the write-in column. By filtering out rows that did not include values within the party detailed column, it caused there to no longer be values that returned ‘True’ within the write-in column and instead only kept write-ins that returned ‘False’ (i.e. only keeping party detailed values that actually had a candidate written in and thus also data for the candidate’s respective political party associated with him or her).  


For future work, we believe it would be interesting to look at an even more detailed view of the breakdown of total votes by including all the possible political parties listed, rather than just the three main ones and the grouped “Other.” This would involve using the “party_detailed” column instead of the “party_simplified” column, and it would involve using a revised approach to pivoting by employing the pivot function rather than the pivot_table function that we used since we would not need to aggregate. For this project, this analysis was not in the scope of our study as our stakeholders asked for a high-level overview of the political party landscape for presidential elections. An even granular approach may prove helpful for clients interested in generating traction, awareness, and victory for particular parties or to identify potentially emerging, more niche parties that could be beneficial and/or a threat in taking away a subset of votes from a traditionally more recognizable party. Additionally, another method that could be applied to this dataset could be to merge another dataset that also considers information about the state, party, and candidate, such as state population or economic data. This would require new columns through applying a dataframe merge, and would be geared towards clients interested in understanding how additional external factors have an impact on election results. 



## References 

MIT Election Data and Science Lab. (2017). U.S. President 1976–2020. Harvard Dataverse. 
https://doi.org/10.7910/DVN/42MVDX. 
Political polarization in the American public. (2014, June 12). Pew Research Center. 
https://www.pewresearch.org/politics/2014/06/12/political-polarization-in-the-american-public/
Westwood, S. J. et. al (2017). The tie that divides: Cross-national evidence of the primacy of partyism. 
European Consortium for Political Research, 57(2), 333-354. https://doi.org/10.1111/1475-6765.12228. 


